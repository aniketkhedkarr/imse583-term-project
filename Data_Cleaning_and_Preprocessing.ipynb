{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "wfiL4WG35bn7",
        "outputId": "6eb7aaee-6b04-4f67-b680-d40e3272d201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Kaggle credentials...\n",
            "Downloading dataset from Kaggle...\n",
            "Unzipping dataset...\n",
            "Scanning for images...\n",
            "Real images found: 95201\n",
            "Fake images found: 95134\n",
            "Organizing files into Train/Validation/Test folders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train/real: 100%|██████████| 66637/66637 [00:11<00:00, 5905.60it/s]\n",
            "train/fake: 100%|██████████| 66590/66590 [00:11<00:00, 5846.15it/s]\n",
            "validation/real: 100%|██████████| 14283/14283 [00:02<00:00, 6994.39it/s]\n",
            "validation/fake: 100%|██████████| 14273/14273 [00:02<00:00, 5119.67it/s]\n",
            "test/real: 100%|██████████| 14281/14281 [00:02<00:00, 5195.20it/s]\n",
            "test/fake: 100%|██████████| 14271/14271 [00:03<00:00, 3757.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping the organized dataset...\n",
            "Starting download to local machine...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d3d00f6-aa56-4cf8-a1d1-a37eb53a50ff\", \"final_dataset_organized.zip\", 1515932905)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation complete. Total files organized: 190335\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- 1. SETUP AND DOWNLOAD FROM KAGGLE ---\n",
        "print(\"Setting up Kaggle credentials...\")\n",
        "kaggle_creds = {\n",
        "    \"username\": \"aniketkhedkar313\",\n",
        "    \"key\": \"2855e7cfe2b95b7859f216820abe155d\"\n",
        "}\n",
        "\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_creds, f)\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "print(\"Downloading dataset from Kaggle...\")\n",
        "exit_code = os.system('kaggle datasets download -d manjilkarki/deepfake-and-real-images')\n",
        "\n",
        "if exit_code != 0:\n",
        "    raise RuntimeError(\"Failed to download dataset.\")\n",
        "\n",
        "print(\"Unzipping dataset...\")\n",
        "os.system('unzip -q -o deepfake-and-real-images.zip -d /content/raw_data')\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "INPUT_ROOT = \"/content/raw_data\"\n",
        "OUTPUT_DIR = \"/content/cleaned_dataset\"\n",
        "SEED = 123\n",
        "\n",
        "# --- 3. SCAN AND SORT FILES ---\n",
        "print(\"Scanning for images...\")\n",
        "all_files = glob.glob(os.path.join(INPUT_ROOT, \"**\", \"*.jpg\"), recursive=True) + \\\n",
        "            glob.glob(os.path.join(INPUT_ROOT, \"**\", \"*.png\"), recursive=True) + \\\n",
        "            glob.glob(os.path.join(INPUT_ROOT, \"**\", \"*.jpeg\"), recursive=True)\n",
        "\n",
        "real_paths = [p for p in all_files if \"real\" in p.lower() and \"fake\" not in p.lower()]\n",
        "fake_paths = [p for p in all_files if \"fake\" in p.lower() and \"real\" not in p.lower()]\n",
        "\n",
        "print(f\"Real images found: {len(real_paths)}\")\n",
        "print(f\"Fake images found: {len(fake_paths)}\")\n",
        "\n",
        "# --- 4. CREATE SPLITS ---\n",
        "def get_splits(paths):\n",
        "    if not paths: return [], [], []\n",
        "    # 70% Train, 15% Val, 15% Test\n",
        "    train_val, test = train_test_split(paths, test_size=0.15, random_state=SEED)\n",
        "    train, val = train_test_split(train_val, test_size=0.1765, random_state=SEED)\n",
        "    return train, val, test\n",
        "\n",
        "real_train, real_val, real_test = get_splits(real_paths)\n",
        "fake_train, fake_val, fake_test = get_splits(fake_paths)\n",
        "\n",
        "splits = {\n",
        "    \"train\": {\"real\": real_train, \"fake\": fake_train},\n",
        "    \"validation\": {\"real\": real_val, \"fake\": fake_val},\n",
        "    \"test\": {\"real\": real_test, \"fake\": fake_test}\n",
        "}\n",
        "\n",
        "# --- 5. COPY FILES (NO PROCESSING) ---\n",
        "print(\"Organizing files into Train/Validation/Test folders...\")\n",
        "if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "\n",
        "file_count = 0\n",
        "\n",
        "for split_name, categories in splits.items():\n",
        "    for class_name, paths in categories.items():\n",
        "        save_dir = os.path.join(OUTPUT_DIR, split_name, class_name)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        for src_path in tqdm(paths, desc=f\"{split_name}/{class_name}\"):\n",
        "            try:\n",
        "                # Direct file copy (Fastest method)\n",
        "                fname = os.path.basename(src_path)\n",
        "                dst_path = os.path.join(save_dir, fname)\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "                file_count += 1\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "# --- 6. ZIP AND DOWNLOAD ---\n",
        "print(\"Zipping the organized dataset...\")\n",
        "shutil.make_archive(\"/content/final_dataset_organized\", 'zip', OUTPUT_DIR)\n",
        "\n",
        "print(\"Starting download to local machine...\")\n",
        "files.download(\"/content/final_dataset_organized.zip\")\n",
        "\n",
        "print(f\"Operation complete. Total files organized: {file_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-TQcWxS5cge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}