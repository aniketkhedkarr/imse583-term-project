{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u18t6CVGuvYK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. SETUP FEATURE EXTRACTOR\n",
        "# We take your existing ResNet model but replace the final \"decision\" layer\n",
        "# with an Identity layer. This lets the raw 512 features pass through.\n",
        "model.fc = nn.Identity()\n",
        "model.eval() # Set to evaluation mode\n",
        "model.to(device)\n",
        "\n",
        "print(\"Feature Extractor Ready. Model will output 512-dimensional vectors.\")\n",
        "\n",
        "# 2. HELPER FUNCTION TO EXTRACT FEATURES\n",
        "def extract_features(loader, model, device):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    print(f\"Extracting features from {len(loader)} batches...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Get the features (vectors) instead of predictions\n",
        "            features = model(images)\n",
        "\n",
        "            # Move to CPU and flatten\n",
        "            features_list.append(features.cpu().numpy())\n",
        "            labels_list.append(labels.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches into one large array\n",
        "    X = np.concatenate(features_list, axis=0)\n",
        "    y = np.concatenate(labels_list, axis=0)\n",
        "    return X, y\n",
        "\n",
        "# 3. EXTRACT DATA\n",
        "# Note: This may take a minute as it processes all images\n",
        "print(\"--- Processing Training Data ---\")\n",
        "X_train, y_train = extract_features(train_loader, model, device)\n",
        "\n",
        "print(\"--- Processing Test Data ---\")\n",
        "X_test, y_test = extract_features(test_loader, model, device)\n",
        "\n",
        "print(f\"Data Loaded! Training Shape: {X_train.shape}\")\n",
        "\n",
        "# 4. TRAIN XGBOOST\n",
        "# We use standard parameters for binary classification\n",
        "print(\"--- Training XGBoost Classifier ---\")\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,      # Number of trees\n",
        "    learning_rate=0.1,     # Step size\n",
        "    max_depth=5,           # Depth of trees\n",
        "    objective='binary:logistic', # For Real vs Fake\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "print(\"Training Complete!\")\n",
        "\n",
        "# 5. EVALUATE\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nFinal XGBoost Accuracy: {acc*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))"
      ]
    }
  ]
}